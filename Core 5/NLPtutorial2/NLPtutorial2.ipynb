{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Create a machine translator for your language of choice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Choose your language pair\n",
    "\n",
    "The first step is to choose the language pair you want to translate. Go to the following website: http://www.manythings.org/anki/. This website has a large collection of bilingual sentence pairs in many different languages. Choose the language pair that you want to work with and download the corresponding data set. For example, if you want to translate from English to French, download the \"French - English\" data set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare the data\n",
    "\n",
    "The next step is to prepare the data for training. This involves preprocessing the data, splitting it into training and testing sets, and converting the text into numerical data that can be fed into the model. You can use Python and the Keras library to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "from keras.models import Model\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "# Read in the dataset\n",
    "data = pd.read_csv(\"afr.txt\", delimiter='\\t', header=None, names=['source', 'target', 'comments'])\n",
    "\n",
    "# Remove comments column\n",
    "data = data[['source', 'target']]\n",
    "\n",
    "# Convert text to lowercase\n",
    "data['source'] = data['source'].apply(lambda x: x.lower())\n",
    "data['target'] = data['target'].apply(lambda x: x.lower())\n",
    "\n",
    "# Tokenize the text\n",
    "source_tokenizer = Tokenizer()\n",
    "source_tokenizer.fit_on_texts(data['source'])\n",
    "target_tokenizer = Tokenizer()\n",
    "target_tokenizer.fit_on_texts(data['target'])\n",
    "\n",
    "# Convert text to sequences of integers\n",
    "source_sequences = source_tokenizer.texts_to_sequences(data['source'])\n",
    "target_sequences = target_tokenizer.texts_to_sequences(data['target'])\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "max_sequence_length = 100\n",
    "source_data = pad_sequences(source_sequences, maxlen=max_sequence_length, padding='post')\n",
    "target_data = pad_sequences(target_sequences, maxlen=max_sequence_length, padding='post')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we first read in the dataset and remove the comments column. We then convert the text to lowercase and tokenize the text using Keras' Tokenizer class. We also pad the sequences to a fixed length of 100.\n",
    "\n",
    "Next, we split the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(source_data, target_data, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our encoder and decoder models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input sequence\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "# Define output sequence\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# Define encoder embedding layer\n",
    "encoder_embedding = Embedding(len(source_tokenizer.word_index) + 1, 256)\n",
    "encoder_embedding_output = encoder_embedding(encoder_inputs)\n",
    "\n",
    "# Define encoder LSTM layer\n",
    "encoder_lstm = LSTM(256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding_output)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define decoder embedding layer\n",
    "decoder_embedding = Embedding(len(target_tokenizer.word_index) + 1, 256)\n",
    "decoder_embedding_output = decoder_embedding(decoder_inputs)\n",
    "\n",
    "# Define decoder LSTM layer\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding_output, initial_state=encoder_states)\n",
    "\n",
    "# Define output layer\n",
    "decoder_dense = Dense(len(target_tokenizer.word_index) + 1, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we define the input and output sequences, as well as the embedding and LSTM layers for the encoder and decoder. We also define the output layer and the entire model.\n",
    "\n",
    "We can now compile and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\heath\\AppData\\Local\\Temp\\ipykernel_14964\\319380665.py:34: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator=generate_batch(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "11/11 [==============================] - 31s 2s/step - loss: 4.2335 - accuracy: 0.8600 - val_loss: 0.6050 - val_accuracy: 0.9481\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 19s 2s/step - loss: 0.4189 - accuracy: 0.9464 - val_loss: 0.3641 - val_accuracy: 0.9481\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 18s 2s/step - loss: 0.3579 - accuracy: 0.9462 - val_loss: 0.3468 - val_accuracy: 0.9481\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 23s 2s/step - loss: 0.3425 - accuracy: 0.9468 - val_loss: 0.3418 - val_accuracy: 0.9481\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 24s 2s/step - loss: 0.3394 - accuracy: 0.9464 - val_loss: 0.3400 - val_accuracy: 0.9481\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 19s 2s/step - loss: 0.3361 - accuracy: 0.9464 - val_loss: 0.3392 - val_accuracy: 0.9481\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 21s 2s/step - loss: 0.3574 - accuracy: 0.9447 - val_loss: 0.3849 - val_accuracy: 0.9431\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 17s 1s/step - loss: 0.3550 - accuracy: 0.9458 - val_loss: 0.3452 - val_accuracy: 0.9505\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 17s 1s/step - loss: 0.3344 - accuracy: 0.9488 - val_loss: 0.3375 - val_accuracy: 0.9510\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 16s 1s/step - loss: 0.3301 - accuracy: 0.9489 - val_loss: 0.3369 - val_accuracy: 0.9508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a2bd6c6710>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "# Define batch size and number of epochs\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "# Define generator for training data\n",
    "def generate_batch(X=X_train, y=y_train, batch_size=batch_size):\n",
    "    while True:\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            encoder_input_data = X[i:i + batch_size]\n",
    "            decoder_input_data = y[i:i + batch_size, :-1]\n",
    "            decoder_output_data = y[i:i + batch_size, 1:]\n",
    "            encoder_input_data = np.array(encoder_input_data)\n",
    "            decoder_input_data = np.array(decoder_input_data)\n",
    "            decoder_output_data = np.array(decoder_output_data)\n",
    "            decoder_output_data = to_categorical(decoder_output_data, num_classes=len(target_tokenizer.word_index) + 1)\n",
    "            yield ([encoder_input_data, decoder_input_data], decoder_output_data)\n",
    "\n",
    "# Define generator for validation data\n",
    "def generate_validation(X=X_val, y=y_val):\n",
    "    encoder_input_data = np.array(X)\n",
    "    decoder_input_data = np.array(y[:, :-1])\n",
    "    decoder_output_data = np.array(y[:, 1:])\n",
    "    decoder_output_data = to_categorical(decoder_output_data, num_classes=len(target_tokenizer.word_index) + 1)\n",
    "    return ([encoder_input_data, decoder_input_data], decoder_output_data)\n",
    "\n",
    "# Train model\n",
    "model.fit_generator(generator=generate_batch(),\n",
    "                    steps_per_epoch=len(X_train)//batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=generate_validation(),\n",
    "                    validation_steps=len(X_val)//batch_size)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we define the batch size and number of epochs. We also define the generators for the training and validation data. The generate_batch function generates batches of data for the training data. The generate_validation function generates data for the validation data. We then train the model using the fit_generator function.\n",
    "\n",
    "Finally, we can make predictions using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define encoder model\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Define decoder inputs\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding_output = decoder_embedding(decoder_inputs)\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding_output, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Define function to decode sequence\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input sequence to get the encoder states\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1\n",
    "    target_seq = np.zeros((1, 1))\n",
    "\n",
    "    # Populate the first character of target sequence with the start character\n",
    "    target_seq[0, 0] = target_tokenizer.word_index['<start>']\n",
    "\n",
    "    # Generate output sequence\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = target_tokenizer.index_word[sampled_token_index]\n",
    "\n",
    "        # Exit condition: either hit max length or find stop character\n",
    "        if (sampled_token == '<end>' or len(decoded_sentence) > max_sequence_length):\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            decoded_sentence += ' ' + sampled_token\n",
    "            \n",
    "            # Update the target sequence\n",
    "            target_seq = np.zeros((1, 1))\n",
    "            target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "            # Update states\n",
    "            states_value = [h, c]\n",
    "\n",
    "        return decoded_sentence\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we define a function sequence_to_text that converts a sequence to text. We then generate some translations for the validation data by selecting a random input sequence, using the decode_sequence function to generate the predicted output sequence, and then converting the sequences to text. We print the input, target, and predicted sequences for each example.\n",
    "\n",
    "That's it! You now have a working sequence-to-sequence model that can be used for machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_text(sequence, tokenizer):\n",
    "    \"\"\"\n",
    "    Converts a sequence of integers to its corresponding text sequence.\n",
    "    \n",
    "    Args:\n",
    "    - sequence (np.array): A sequence of integers.\n",
    "    - tokenizer (keras.preprocessing.text.Tokenizer): A tokenizer fitted on the text data.\n",
    "    \n",
    "    Returns:\n",
    "    - A string representing the text sequence.\n",
    "    \"\"\"\n",
    "    text = tokenizer.sequences_to_texts([sequence])[0]\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\heath\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\heath\\Anaconda3\\envs\\chessEnv\\lib\\site-packages\\keras\\engine\\training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\heath\\Anaconda3\\envs\\chessEnv\\lib\\site-packages\\keras\\engine\\training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\heath\\Anaconda3\\envs\\chessEnv\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\heath\\Anaconda3\\envs\\chessEnv\\lib\\site-packages\\keras\\engine\\training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\heath\\Anaconda3\\envs\\chessEnv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\heath\\Anaconda3\\envs\\chessEnv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 100) dtype=int32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m input_seq \u001b[39m=\u001b[39m X_val[i:i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     23\u001b[0m actual_output_seq \u001b[39m=\u001b[39m y_val[i:i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m---> 24\u001b[0m predicted_output_seq \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(input_seq)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     25\u001b[0m bleu_score \u001b[39m=\u001b[39m calculate_bleu_score(actual_output_seq, predicted_output_seq)\n\u001b[0;32m     26\u001b[0m total_bleu_score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m bleu_score\n",
      "File \u001b[1;32mc:\\Users\\heath\\Anaconda3\\envs\\chessEnv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filey_3s1ltf.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\heath\\Anaconda3\\envs\\chessEnv\\lib\\site-packages\\keras\\engine\\training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\heath\\Anaconda3\\envs\\chessEnv\\lib\\site-packages\\keras\\engine\\training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\heath\\Anaconda3\\envs\\chessEnv\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\heath\\Anaconda3\\envs\\chessEnv\\lib\\site-packages\\keras\\engine\\training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\heath\\Anaconda3\\envs\\chessEnv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\heath\\Anaconda3\\envs\\chessEnv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 100) dtype=int32>]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def calculate_bleu_score(actual, predicted):\n",
    "    # Convert the sequences to text\n",
    "    actual_text = sequence_to_text(actual, target_tokenizer)\n",
    "    predicted_text = sequence_to_text(predicted, target_tokenizer)\n",
    "\n",
    "    # Tokenize the text into individual words\n",
    "    actual_words = nltk.word_tokenize(actual_text.lower())\n",
    "    predicted_words = nltk.word_tokenize(predicted_text.lower())\n",
    "\n",
    "    # Calculate the BLEU score\n",
    "    weights = [(1.0 / i) for i in range(1, 5)]\n",
    "    bleu_score = nltk.translate.bleu_score.sentence_bleu([actual_words], predicted_words, weights)\n",
    "\n",
    "    return bleu_score\n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "total_bleu_score = 0\n",
    "for i in range(len(X_val)):\n",
    "    input_seq = X_val[i:i+1]\n",
    "    actual_output_seq = y_val[i:i+1]\n",
    "    predicted_output_seq = model.predict(input_seq)[0]\n",
    "    bleu_score = calculate_bleu_score(actual_output_seq, predicted_output_seq)\n",
    "    total_bleu_score += bleu_score\n",
    "\n",
    "average_bleu_score = total_bleu_score / len(X_val)\n",
    "print(\"Average BLEU score on validation data:\", average_bleu_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
