{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Create a machine translator for your language of choice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Choose your language pair\n",
    "\n",
    "The first step is to choose the language pair you want to translate. Go to the following website: http://www.manythings.org/anki/. This website has a large collection of bilingual sentence pairs in many different languages. Choose the language pair that you want to work with and download the corresponding data set. For example, if you want to translate from English to French, download the \"French - English\" data set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare the data\n",
    "\n",
    "The next step is to prepare the data for training. This involves preprocessing the data, splitting it into training and testing sets, and converting the text into numerical data that can be fed into the model. You can use Python and the Keras library to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "from keras.models import Model\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "# Read in the dataset\n",
    "data = pd.read_csv(\"afr.txt\", delimiter='\\t', header=None, names=['source', 'target', 'comments'])\n",
    "\n",
    "# Remove comments column\n",
    "data = data[['source', 'target']]\n",
    "\n",
    "# Convert text to lowercase\n",
    "data['source'] = data['source'].apply(lambda x: x.lower())\n",
    "data['target'] = data['target'].apply(lambda x: x.lower())\n",
    "\n",
    "# Tokenize the text\n",
    "source_tokenizer = Tokenizer()\n",
    "source_tokenizer.fit_on_texts(data['source'])\n",
    "target_tokenizer = Tokenizer()\n",
    "target_tokenizer.fit_on_texts(data['target'])\n",
    "\n",
    "# Convert text to sequences of integers\n",
    "source_sequences = source_tokenizer.texts_to_sequences(data['source'])\n",
    "target_sequences = target_tokenizer.texts_to_sequences(data['target'])\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "max_sequence_length = 100\n",
    "source_data = pad_sequences(source_sequences, maxlen=max_sequence_length, padding='post')\n",
    "target_data = pad_sequences(target_sequences, maxlen=max_sequence_length, padding='post')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we first read in the dataset and remove the comments column. We then convert the text to lowercase and tokenize the text using Keras' Tokenizer class. We also pad the sequences to a fixed length of 100.\n",
    "\n",
    "Next, we split the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(source_data, target_data, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our encoder and decoder models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input sequence\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "# Define output sequence\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# Define encoder embedding layer\n",
    "encoder_embedding = Embedding(len(source_tokenizer.word_index) + 1, 256)\n",
    "encoder_embedding_output = encoder_embedding(encoder_inputs)\n",
    "\n",
    "# Define encoder LSTM layer\n",
    "encoder_lstm = LSTM(256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding_output)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define decoder embedding layer\n",
    "decoder_embedding = Embedding(len(target_tokenizer.word_index) + 1, 256)\n",
    "decoder_embedding_output = decoder_embedding(decoder_inputs)\n",
    "\n",
    "# Define decoder LSTM layer\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding_output, initial_state=encoder_states)\n",
    "\n",
    "# Define output layer\n",
    "decoder_dense = Dense(len(target_tokenizer.word_index) + 1, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we define the input and output sequences, as well as the embedding and LSTM layers for the encoder and decoder. We also define the output layer and the entire model.\n",
    "\n",
    "We can now compile and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Warmtebron\\AppData\\Local\\Temp\\ipykernel_10028\\319380665.py:34: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator=generate_batch(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 9s 619ms/step - loss: 4.6631 - accuracy: 0.8598 - val_loss: 0.9156 - val_accuracy: 0.9481\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 6s 579ms/step - loss: 0.4749 - accuracy: 0.9464 - val_loss: 0.3634 - val_accuracy: 0.9481\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 6s 565ms/step - loss: 0.3641 - accuracy: 0.9462 - val_loss: 0.3492 - val_accuracy: 0.9481\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 6s 562ms/step - loss: 0.3442 - accuracy: 0.9468 - val_loss: 0.3427 - val_accuracy: 0.9481\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 6s 566ms/step - loss: 0.3393 - accuracy: 0.9464 - val_loss: 0.3400 - val_accuracy: 0.9481\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 6s 559ms/step - loss: 0.3355 - accuracy: 0.9464 - val_loss: 0.3385 - val_accuracy: 0.9481\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 6s 561ms/step - loss: 0.3308 - accuracy: 0.9486 - val_loss: 0.3369 - val_accuracy: 0.9514\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 6s 555ms/step - loss: 0.3281 - accuracy: 0.9496 - val_loss: 0.3795 - val_accuracy: 0.9460\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 6s 552ms/step - loss: 0.3395 - accuracy: 0.9481 - val_loss: 0.3385 - val_accuracy: 0.9506\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 6s 550ms/step - loss: 0.3290 - accuracy: 0.9492 - val_loss: 0.3372 - val_accuracy: 0.9507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f0421e4130>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "# Define batch size and number of epochs\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "# Define generator for training data\n",
    "def generate_batch(X=X_train, y=y_train, batch_size=batch_size):\n",
    "    while True:\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            encoder_input_data = X[i:i + batch_size]\n",
    "            decoder_input_data = y[i:i + batch_size, :-1]\n",
    "            decoder_output_data = y[i:i + batch_size, 1:]\n",
    "            encoder_input_data = np.array(encoder_input_data)\n",
    "            decoder_input_data = np.array(decoder_input_data)\n",
    "            decoder_output_data = np.array(decoder_output_data)\n",
    "            decoder_output_data = to_categorical(decoder_output_data, num_classes=len(target_tokenizer.word_index) + 1)\n",
    "            yield ([encoder_input_data, decoder_input_data], decoder_output_data)\n",
    "\n",
    "# Define generator for validation data\n",
    "def generate_validation(X=X_val, y=y_val):\n",
    "    encoder_input_data = np.array(X)\n",
    "    decoder_input_data = np.array(y[:, :-1])\n",
    "    decoder_output_data = np.array(y[:, 1:])\n",
    "    decoder_output_data = to_categorical(decoder_output_data, num_classes=len(target_tokenizer.word_index) + 1)\n",
    "    return ([encoder_input_data, decoder_input_data], decoder_output_data)\n",
    "\n",
    "# Train model\n",
    "model.fit_generator(generator=generate_batch(),\n",
    "                    steps_per_epoch=len(X_train)//batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=generate_validation(),\n",
    "                    validation_steps=len(X_val)//batch_size)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we define the batch size and number of epochs. We also define the generators for the training and validation data. The generate_batch function generates batches of data for the training data. The generate_validation function generates data for the validation data. We then train the model using the fit_generator function.\n",
    "\n",
    "Finally, we can make predictions using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define encoder model\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Define decoder inputs\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding_output = decoder_embedding(decoder_inputs)\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding_output, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Define function to decode sequence\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input sequence to get the encoder states\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1\n",
    "    target_seq = np.zeros((1, 1))\n",
    "\n",
    "    # Populate the first character of target sequence with the start character\n",
    "    target_seq[0, 0] = target_tokenizer.word_index['<start>']\n",
    "\n",
    "    # Generate output sequence\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = target_tokenizer.index_word[sampled_token_index]\n",
    "\n",
    "        # Exit condition: either hit max length or find stop character\n",
    "        if (sampled_token == '<end>' or len(decoded_sentence) > max_sequence_length):\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            decoded_sentence += ' ' + sampled_token\n",
    "            \n",
    "            # Update the target sequence\n",
    "            target_seq = np.zeros((1, 1))\n",
    "            target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "            # Update states\n",
    "            states_value = [h, c]\n",
    "\n",
    "        return decoded_sentence\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we define a function sequence_to_text that converts a sequence to text. We then generate some translations for the validation data by selecting a random input sequence, using the decode_sequence function to generate the predicted output sequence, and then converting the sequences to text. We print the input, target, and predicted sequences for each example.\n",
    "\n",
    "That's it! You now have a working sequence-to-sequence model that can be used for machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_text(sequence, tokenizer):\n",
    "    \"\"\"\n",
    "    Converts a sequence of integers to its corresponding text sequence.\n",
    "    \n",
    "    Args:\n",
    "    - sequence (np.array): A sequence of integers.\n",
    "    - tokenizer (keras.preprocessing.text.Tokenizer): A tokenizer fitted on the text data.\n",
    "    \n",
    "    Returns:\n",
    "    - A string representing the text sequence.\n",
    "    \"\"\"\n",
    "    text = tokenizer.sequences_to_texts([sequence])[0]\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>come in.</td>\n",
       "      <td>gaan binne.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i'm full.</td>\n",
       "      <td>ek is vol.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>she runs.</td>\n",
       "      <td>sy hardloop.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you lost.</td>\n",
       "      <td>jy verloor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>go inside.</td>\n",
       "      <td>gaan binne.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       source        target\n",
       "0    come in.   gaan binne.\n",
       "1   i'm full.    ek is vol.\n",
       "2   she runs.  sy hardloop.\n",
       "3   you lost.   jy verloor.\n",
       "4  go inside.   gaan binne."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n",
      "Review: come in\n",
      "Predicted sentiment: het\n"
     ]
    }
   ],
   "source": [
    "# Define a new review\n",
    "new_review = \"come in\"\n",
    "\n",
    "# Convert the review to a sequence of integers\n",
    "new_review_seq = source_tokenizer.texts_to_sequences([new_review])[0]\n",
    "\n",
    "# Pad the sequence to the same length as the training data\n",
    "new_review_seq = pad_sequences([new_review_seq], maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Use the model to predict the sentiment of the review\n",
    "pred_sentiment_seq = model.predict([new_review_seq, np.zeros((len(new_review_seq), 1))])\n",
    "\n",
    "# Convert the predicted sentiment sequence to text\n",
    "pred_sentiment_text = sequence_to_text(np.argmax(pred_sentiment_seq, axis=2)[0], target_tokenizer)\n",
    "\n",
    "print(\"Review:\", new_review)\n",
    "print(\"Predicted sentiment:\", pred_sentiment_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
