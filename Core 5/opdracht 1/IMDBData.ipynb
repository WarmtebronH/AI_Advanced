{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP project\n",
    "\n",
    "In this project I will use NLP on the IMDB dataset. The first step is to read this dataset and prepare it for the NLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "from keras.models import Model\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "# Read in the dataset\n",
    "data = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to lowercase\n",
    "data['review'] = data['review'].apply(lambda x: x.lower())\n",
    "\n",
    "# Tokenize the text\n",
    "review_tokenizer = Tokenizer()\n",
    "review_tokenizer.fit_on_texts(data['review'])\n",
    "sentiment_tokenizer = Tokenizer()\n",
    "sentiment_tokenizer.fit_on_texts(data['sentiment'])\n",
    "\n",
    "# Convert text to sequences of integers\n",
    "review_sequences = review_tokenizer.texts_to_sequences(data['review'])\n",
    "sentiment_sequences = sentiment_tokenizer.texts_to_sequences(data['sentiment'])\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "max_sequence_length = 100\n",
    "review_data = pad_sequences(review_sequences, maxlen=max_sequence_length, padding='post')\n",
    "sentiment_data = pad_sequences(sentiment_sequences, maxlen=max_sequence_length, padding='post')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we first read in the dataset. We then convert the text to lowercase and tokenize the text using Keras' Tokenizer class. We also pad the sequences to a fixed length of 100.\n",
    "\n",
    "Next, we split the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   34,  1637,     9, ...,   125,  4103,   486],\n",
       "       [ 9719,    31,     1, ...,  1977,    69,   221],\n",
       "       [ 3059,    12,  2971, ...,    63,    16,   350],\n",
       "       ...,\n",
       "       [   26,     3,  1156, ..., 22840,     2,  6050],\n",
       "       [    5,    68,   135, ...,    67,   739,    42],\n",
       "       [  699,   479,    11, ...,   794,    11,    17]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [2, 0, 0, ..., 0, 0, 0],\n",
       "       [2, 0, 0, ..., 0, 0, 0],\n",
       "       [2, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(review_data, sentiment_data, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our encoder and decoder models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input sequence\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "# Define output sequence\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# Define encoder embedding layer\n",
    "encoder_embedding = Embedding(len(review_tokenizer.word_index) + 1, 256)\n",
    "encoder_embedding_output = encoder_embedding(encoder_inputs)\n",
    "\n",
    "# Define encoder LSTM layer\n",
    "encoder_lstm = LSTM(256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding_output)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define decoder embedding layer\n",
    "decoder_embedding = Embedding(len(sentiment_tokenizer.word_index) + 1, 256)\n",
    "decoder_embedding_output = decoder_embedding(decoder_inputs)\n",
    "\n",
    "# Define decoder LSTM layer\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding_output, initial_state=encoder_states)\n",
    "\n",
    "# Define output layer\n",
    "decoder_dense = Dense(len(sentiment_tokenizer.word_index) + 1, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we define the input and output sequences, as well as the embedding and LSTM layers for the encoder and decoder. We also define the output layer and the entire model.\n",
    "\n",
    "We can now compile and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\heath\\AppData\\Local\\Temp\\ipykernel_22244\\2935895805.py:34: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator=generate_batch(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.0055 - accuracy: 0.9984WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156 batches). You may need to use the repeat() function when building your dataset.\n",
      "625/625 [==============================] - 1407s 2s/step - loss: 0.0055 - accuracy: 0.9984 - val_loss: 2.0338e-08 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "295/625 [=============>................] - ETA: 10:55 - loss: 1.8105e-08 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "# Define batch size and number of epochs\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "# Define generator for training data\n",
    "def generate_batch(X=X_train, y=y_train, batch_size=batch_size):\n",
    "    while True:\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            encoder_input_data = X[i:i + batch_size]\n",
    "            decoder_input_data = y[i:i + batch_size, :-1]\n",
    "            decoder_output_data = y[i:i + batch_size, 1:]\n",
    "            encoder_input_data = np.array(encoder_input_data)\n",
    "            decoder_input_data = np.array(decoder_input_data)\n",
    "            decoder_output_data = np.array(decoder_output_data)\n",
    "            decoder_output_data = to_categorical(decoder_output_data, num_classes=len(sentiment_tokenizer.word_index) + 1)\n",
    "            yield ([encoder_input_data, decoder_input_data], decoder_output_data)\n",
    "\n",
    "# Define generator for validation data\n",
    "def generate_validation(X=X_val, y=y_val):\n",
    "    encoder_input_data = np.array(X)\n",
    "    decoder_input_data = np.array(y[:, :-1])\n",
    "    decoder_output_data = np.array(y[:, 1:])\n",
    "    decoder_output_data = to_categorical(decoder_output_data, num_classes=len(sentiment_tokenizer.word_index) + 1)\n",
    "    return ([encoder_input_data, decoder_input_data], decoder_output_data)\n",
    "\n",
    "# Train model\n",
    "model.fit_generator(generator=generate_batch(),\n",
    "                    steps_per_epoch=len(X_train)//batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=generate_validation(),\n",
    "                    validation_steps=len(X_val)//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chessEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
