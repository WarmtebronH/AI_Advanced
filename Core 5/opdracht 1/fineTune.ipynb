{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning a transformer model\n",
    "\n",
    "For this assignment I am using the imdb dataset I found on kaggle. I could not get the dataset from huggingface imported due to a server error. Hence I did this assignment with a different dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data understanding\n",
    "\n",
    "I'll start with importing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset from the local file\n",
    "df = pd.read_csv(\"imdb dataset.csv\")\n",
    "\n",
    "# Preview the first few rows\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then I look at the shape of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has 50000 rows and 2 columns. The columns this dataset has are:\n",
    "- review\n",
    "- sentiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am now going to create a brief overview of the dataset using the describe() function. The describe function provides a quick and useful summary of some important points. This shows how many different unique values each column has and which value appears most frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review sentiment\n",
       "count                                               50000     50000\n",
       "unique                                              49582         2\n",
       "top     Loved today's show!!! It was a variety and not...  positive\n",
       "freq                                                    5     25000"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these two columns, we can infer the following information:\n",
    "\n",
    "- The dataset contains 50,000 reviews in total.\n",
    "- The \"review\" column has 49,582 unique reviews, indicating that some reviews appear multiple times in the dataset.\n",
    "- The \"sentiment\" column has 2 unique values, indicating that the sentiment of the reviews is either positive or negative.\n",
    "- The most common sentiment in the dataset is positive, with 25,000 reviews classified as such.\n",
    "- The top review that appears multiple times in the dataset is \"Loved today's show!!! It was a variety and not...\", which appears 5 times. However, we do not know whether these instances of the review are classified as positive or negative, as this information is not provided in the summary."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning the data\n",
    "\n",
    "In this part, I will examine whether there are any missing values in the dataset and whether any rows or columns can be removed. Missing values refer to the absence of data in certain cells, and they can occur for various reasons such as errors in data collection or data entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no values missing in this dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenize the data with NLTK\n",
    "\n",
    "Now I'll start with tokenizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command \"nltk.download('punkt');\" will initiate the NLTK downloader and instruct it to install the punkt data, which is a sentence tokenizer that takes a sentence of words and breaks it down into individual tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Warmtebron\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function for tokenization\n",
    "def tokenize(column):\n",
    "    tokens = nltk.word_tokenize(column)\n",
    "    return [w for w in tokens if w.isalpha()] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use our function on the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[One, of, the, other, reviewers, has, mentione...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[A, wonderful, little, production, br, br, The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[I, thought, this, was, a, wonderful, way, to,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Basically, there, a, family, where, a, little...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Petter, Mattei, Love, in, the, Time, of, Mone...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tokenized\n",
       "0  [One, of, the, other, reviewers, has, mentione...\n",
       "1  [A, wonderful, little, production, br, br, The...\n",
       "2  [I, thought, this, was, a, wonderful, way, to,...\n",
       "3  [Basically, there, a, family, where, a, little...\n",
       "4  [Petter, Mattei, Love, in, the, Time, of, Mone..."
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenized'] = df.apply(lambda x: tokenize(x['review']), axis=1)\n",
    "df[['tokenized']].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Review Encoden\n",
    "\n",
    "To encode the tokenized data, I will assign a numerical value to each word that appears in the review column. Then, I will pad the encoded data to ensure that all sequences of numbers are the same length. Padding involves adding zeros to the end of each sequence so that they have the same length as the longest sequence in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[One, of, the, other, reviewers, has, mentione...</td>\n",
       "      <td>[28, 4, 1, 79, 1940, 45, 1025, 12, 99, 142, 40...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[A, wonderful, little, production, br, br, The...</td>\n",
       "      <td>[3, 370, 118, 351, 7, 7, 1, 1321, 2928, 6, 52,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[I, thought, this, was, a, wonderful, way, to,...</td>\n",
       "      <td>[10, 187, 11, 13, 3, 370, 96, 5, 1072, 60, 21,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[Basically, there, a, family, where, a, little...</td>\n",
       "      <td>[641, 38, 3, 223, 111, 3, 118, 405, 3192, 1166...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[Petter, Mattei, Love, in, the, Time, of, Mone...</td>\n",
       "      <td>[60692, 10349, 112, 9, 1, 60, 4, 281, 6, 3, 20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [One, of, the, other, reviewers, has, mentione...   \n",
       "1  [A, wonderful, little, production, br, br, The...   \n",
       "2  [I, thought, this, was, a, wonderful, way, to,...   \n",
       "3  [Basically, there, a, family, where, a, little...   \n",
       "4  [Petter, Mattei, Love, in, the, Time, of, Mone...   \n",
       "\n",
       "                                           sequences  \n",
       "0  [28, 4, 1, 79, 1940, 45, 1025, 12, 99, 142, 40...  \n",
       "1  [3, 370, 118, 351, 7, 7, 1, 1321, 2928, 6, 52,...  \n",
       "2  [10, 187, 11, 13, 3, 370, 96, 5, 1072, 60, 21,...  \n",
       "3  [641, 38, 3, 223, 111, 3, 118, 405, 3192, 1166...  \n",
       "4  [60692, 10349, 112, 9, 1, 60, 4, 281, 6, 3, 20...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# De trefwoorden tokenizen\n",
    "t  = Tokenizer()\n",
    "t.fit_on_texts(df['tokenized'])\n",
    "df['sequences'] = t.texts_to_sequences(df['tokenized'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2444</th>\n",
       "      <th>2445</th>\n",
       "      <th>2446</th>\n",
       "      <th>2447</th>\n",
       "      <th>2448</th>\n",
       "      <th>2449</th>\n",
       "      <th>2450</th>\n",
       "      <th>2451</th>\n",
       "      <th>2452</th>\n",
       "      <th>2453</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[28</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>1940</td>\n",
       "      <td>45</td>\n",
       "      <td>1025</td>\n",
       "      <td>12</td>\n",
       "      <td>99</td>\n",
       "      <td>142</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[3</td>\n",
       "      <td>370</td>\n",
       "      <td>118</td>\n",
       "      <td>351</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1321</td>\n",
       "      <td>2928</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[10</td>\n",
       "      <td>187</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>370</td>\n",
       "      <td>96</td>\n",
       "      <td>5</td>\n",
       "      <td>1072</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[641</td>\n",
       "      <td>38</td>\n",
       "      <td>3</td>\n",
       "      <td>223</td>\n",
       "      <td>111</td>\n",
       "      <td>3</td>\n",
       "      <td>118</td>\n",
       "      <td>405</td>\n",
       "      <td>3192</td>\n",
       "      <td>1166</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[60692</td>\n",
       "      <td>10349</td>\n",
       "      <td>112</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>281</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>[10</td>\n",
       "      <td>187</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>180</td>\n",
       "      <td>195</td>\n",
       "      <td>49</td>\n",
       "      <td>282</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>[84</td>\n",
       "      <td>114</td>\n",
       "      <td>84</td>\n",
       "      <td>392</td>\n",
       "      <td>84</td>\n",
       "      <td>113</td>\n",
       "      <td>2888</td>\n",
       "      <td>926</td>\n",
       "      <td>1</td>\n",
       "      <td>605</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>[10</td>\n",
       "      <td>222</td>\n",
       "      <td>3</td>\n",
       "      <td>3380</td>\n",
       "      <td>4195</td>\n",
       "      <td>9</td>\n",
       "      <td>36866</td>\n",
       "      <td>8061</td>\n",
       "      <td>5260</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>[10</td>\n",
       "      <td>159</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>2930</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>858</td>\n",
       "      <td>875</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>[57</td>\n",
       "      <td>28</td>\n",
       "      <td>5596</td>\n",
       "      <td>1</td>\n",
       "      <td>328</td>\n",
       "      <td>2005</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>310</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2454 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1      2      3      4      5       6      7      8     \\\n",
       "0         [28       4      1     79   1940     45    1025     12     99   \n",
       "1          [3     370    118    351      7      7       1   1321   2928   \n",
       "2         [10     187     11     13      3    370      96      5   1072   \n",
       "3        [641      38      3    223    111      3     118    405   3192   \n",
       "4      [60692   10349    112      9      1     60       4    281      6   \n",
       "...       ...     ...    ...    ...    ...    ...     ...    ...    ...   \n",
       "49995     [10     187     11     17     70      3     180    195     49   \n",
       "49996     [84     114     84    392     84    113    2888    926      1   \n",
       "49997     [10     222      3   3380   4195      9   36866   8061   5260   \n",
       "49998     [10     159      5     26      5   2930      15      1    858   \n",
       "49999     [57      28   5596      1    328   2005     100      5     27   \n",
       "\n",
       "        9     ... 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453  \n",
       "0        142  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1          6  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "2         60  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "3       1166  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "4          3  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "49995    282  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "49996    605  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "49997     32  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "49998    875  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "49999    310  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[50000 rows x 2454 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Seperating all the numbers\n",
    "df['sequences']. apply(lambda x: pd.Series(str(x).split(\",\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a maximum of 2454 numbers in the data. I can use this number as maxlen value in the next code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 100)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "#Padden\n",
    "input_ids = pad_sequences(\n",
    "    df['sequences'], maxlen=100, dtype=\"long\", truncating=\"post\", padding=\"post\"\n",
    ")\n",
    "\n",
    "#print(input_ids)\n",
    "\n",
    "np.array(input_ids).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[One, of, the, other, reviewers, has, mentione...</td>\n",
       "      <td>[28, 4, 1, 79, 1940, 45, 1025, 12, 99, 142, 40...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[A, wonderful, little, production, br, br, The...</td>\n",
       "      <td>[3, 370, 118, 351, 7, 7, 1, 1321, 2928, 6, 52,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[I, thought, this, was, a, wonderful, way, to,...</td>\n",
       "      <td>[10, 187, 11, 13, 3, 370, 96, 5, 1072, 60, 21,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[Basically, there, a, family, where, a, little...</td>\n",
       "      <td>[641, 38, 3, 223, 111, 3, 118, 405, 3192, 1166...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[Petter, Mattei, Love, in, the, Time, of, Mone...</td>\n",
       "      <td>[60692, 10349, 112, 9, 1, 60, 4, 281, 6, 3, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[I, thought, this, movie, did, a, down, right,...</td>\n",
       "      <td>[10, 187, 11, 17, 70, 3, 180, 195, 49, 282, 8,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[Bad, plot, bad, dialogue, bad, acting, idioti...</td>\n",
       "      <td>[84, 114, 84, 392, 84, 113, 2888, 926, 1, 605,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[I, am, a, Catholic, taught, in, parochial, el...</td>\n",
       "      <td>[10, 222, 3, 3380, 4195, 9, 36866, 8061, 5260,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[I, going, to, have, to, disagree, with, the, ...</td>\n",
       "      <td>[10, 159, 5, 26, 5, 2930, 15, 1, 858, 875, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[No, one, expects, the, Star, Trek, movies, to...</td>\n",
       "      <td>[57, 28, 5596, 1, 328, 2005, 100, 5, 27, 310, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment  \\\n",
       "0      One of the other reviewers has mentioned that ...  positive   \n",
       "1      A wonderful little production. <br /><br />The...  positive   \n",
       "2      I thought this was a wonderful way to spend ti...  positive   \n",
       "3      Basically there's a family where a little boy ...  negative   \n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "...                                                  ...       ...   \n",
       "49995  I thought this movie did a down right good job...  positive   \n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative   \n",
       "49997  I am a Catholic taught in parochial elementary...  negative   \n",
       "49998  I'm going to have to disagree with the previou...  negative   \n",
       "49999  No one expects the Star Trek movies to be high...  negative   \n",
       "\n",
       "                                               tokenized  \\\n",
       "0      [One, of, the, other, reviewers, has, mentione...   \n",
       "1      [A, wonderful, little, production, br, br, The...   \n",
       "2      [I, thought, this, was, a, wonderful, way, to,...   \n",
       "3      [Basically, there, a, family, where, a, little...   \n",
       "4      [Petter, Mattei, Love, in, the, Time, of, Mone...   \n",
       "...                                                  ...   \n",
       "49995  [I, thought, this, movie, did, a, down, right,...   \n",
       "49996  [Bad, plot, bad, dialogue, bad, acting, idioti...   \n",
       "49997  [I, am, a, Catholic, taught, in, parochial, el...   \n",
       "49998  [I, going, to, have, to, disagree, with, the, ...   \n",
       "49999  [No, one, expects, the, Star, Trek, movies, to...   \n",
       "\n",
       "                                               sequences  \n",
       "0      [28, 4, 1, 79, 1940, 45, 1025, 12, 99, 142, 40...  \n",
       "1      [3, 370, 118, 351, 7, 7, 1, 1321, 2928, 6, 52,...  \n",
       "2      [10, 187, 11, 13, 3, 370, 96, 5, 1072, 60, 21,...  \n",
       "3      [641, 38, 3, 223, 111, 3, 118, 405, 3192, 1166...  \n",
       "4      [60692, 10349, 112, 9, 1, 60, 4, 281, 6, 3, 20...  \n",
       "...                                                  ...  \n",
       "49995  [10, 187, 11, 17, 70, 3, 180, 195, 49, 282, 8,...  \n",
       "49996  [84, 114, 84, 392, 84, 113, 2888, 926, 1, 605,...  \n",
       "49997  [10, 222, 3, 3380, 4195, 9, 36866, 8061, 5260,...  \n",
       "49998  [10, 159, 5, 26, 5, 2930, 15, 1, 858, 875, 2, ...  \n",
       "49999  [57, 28, 5596, 1, 328, 2005, 100, 5, 27, 310, ...  \n",
       "\n",
       "[50000 rows x 4 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replacing the sequence column with the new sequence\n",
    "input_ids = input_ids.tolist()\n",
    "df['sequences'] = input_ids\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sentiment Encoden\n",
    "\n",
    "In this step I will encode the sentiments. Because you only have 2 sentiments, the labels will be 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>sequences</th>\n",
       "      <th>Label Encode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[One, of, the, other, reviewers, has, mentione...</td>\n",
       "      <td>[28, 4, 1, 79, 1940, 45, 1025, 12, 99, 142, 40...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[A, wonderful, little, production, br, br, The...</td>\n",
       "      <td>[3, 370, 118, 351, 7, 7, 1, 1321, 2928, 6, 52,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[I, thought, this, was, a, wonderful, way, to,...</td>\n",
       "      <td>[10, 187, 11, 13, 3, 370, 96, 5, 1072, 60, 21,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[Basically, there, a, family, where, a, little...</td>\n",
       "      <td>[641, 38, 3, 223, 111, 3, 118, 405, 3192, 1166...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[Petter, Mattei, Love, in, the, Time, of, Mone...</td>\n",
       "      <td>[60692, 10349, 112, 9, 1, 60, 4, 281, 6, 3, 20...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[I, thought, this, movie, did, a, down, right,...</td>\n",
       "      <td>[10, 187, 11, 17, 70, 3, 180, 195, 49, 282, 8,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[Bad, plot, bad, dialogue, bad, acting, idioti...</td>\n",
       "      <td>[84, 114, 84, 392, 84, 113, 2888, 926, 1, 605,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[I, am, a, Catholic, taught, in, parochial, el...</td>\n",
       "      <td>[10, 222, 3, 3380, 4195, 9, 36866, 8061, 5260,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[I, going, to, have, to, disagree, with, the, ...</td>\n",
       "      <td>[10, 159, 5, 26, 5, 2930, 15, 1, 858, 875, 2, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[No, one, expects, the, Star, Trek, movies, to...</td>\n",
       "      <td>[57, 28, 5596, 1, 328, 2005, 100, 5, 27, 310, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment  \\\n",
       "0      One of the other reviewers has mentioned that ...  positive   \n",
       "1      A wonderful little production. <br /><br />The...  positive   \n",
       "2      I thought this was a wonderful way to spend ti...  positive   \n",
       "3      Basically there's a family where a little boy ...  negative   \n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "...                                                  ...       ...   \n",
       "49995  I thought this movie did a down right good job...  positive   \n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative   \n",
       "49997  I am a Catholic taught in parochial elementary...  negative   \n",
       "49998  I'm going to have to disagree with the previou...  negative   \n",
       "49999  No one expects the Star Trek movies to be high...  negative   \n",
       "\n",
       "                                               tokenized  \\\n",
       "0      [One, of, the, other, reviewers, has, mentione...   \n",
       "1      [A, wonderful, little, production, br, br, The...   \n",
       "2      [I, thought, this, was, a, wonderful, way, to,...   \n",
       "3      [Basically, there, a, family, where, a, little...   \n",
       "4      [Petter, Mattei, Love, in, the, Time, of, Mone...   \n",
       "...                                                  ...   \n",
       "49995  [I, thought, this, movie, did, a, down, right,...   \n",
       "49996  [Bad, plot, bad, dialogue, bad, acting, idioti...   \n",
       "49997  [I, am, a, Catholic, taught, in, parochial, el...   \n",
       "49998  [I, going, to, have, to, disagree, with, the, ...   \n",
       "49999  [No, one, expects, the, Star, Trek, movies, to...   \n",
       "\n",
       "                                               sequences  Label Encode  \n",
       "0      [28, 4, 1, 79, 1940, 45, 1025, 12, 99, 142, 40...             1  \n",
       "1      [3, 370, 118, 351, 7, 7, 1, 1321, 2928, 6, 52,...             1  \n",
       "2      [10, 187, 11, 13, 3, 370, 96, 5, 1072, 60, 21,...             1  \n",
       "3      [641, 38, 3, 223, 111, 3, 118, 405, 3192, 1166...             0  \n",
       "4      [60692, 10349, 112, 9, 1, 60, 4, 281, 6, 3, 20...             1  \n",
       "...                                                  ...           ...  \n",
       "49995  [10, 187, 11, 17, 70, 3, 180, 195, 49, 282, 8,...             1  \n",
       "49996  [84, 114, 84, 392, 84, 113, 2888, 926, 1, 605,...             0  \n",
       "49997  [10, 222, 3, 3380, 4195, 9, 36866, 8061, 5260,...             0  \n",
       "49998  [10, 159, 5, 26, 5, 2930, 15, 1, 858, 875, 2, ...             0  \n",
       "49999  [57, 28, 5596, 1, 328, 2005, 100, 5, 27, 310, ...             0  \n",
       "\n",
       "[50000 rows x 5 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Creating instance of labelencoder\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "# Assigning numerical values and storing in another column\n",
    "df['Label Encode'] = labelencoder.fit_transform(df['sentiment'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label Encode</th>\n",
       "      <th>sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[28, 4, 1, 79, 1940, 45, 1025, 12, 99, 142, 40...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[3, 370, 118, 351, 7, 7, 1, 1321, 2928, 6, 52,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[10, 187, 11, 13, 3, 370, 96, 5, 1072, 60, 21,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[641, 38, 3, 223, 111, 3, 118, 405, 3192, 1166...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[60692, 10349, 112, 9, 1, 60, 4, 281, 6, 3, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>1</td>\n",
       "      <td>[10, 187, 11, 17, 70, 3, 180, 195, 49, 282, 8,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>0</td>\n",
       "      <td>[84, 114, 84, 392, 84, 113, 2888, 926, 1, 605,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>0</td>\n",
       "      <td>[10, 222, 3, 3380, 4195, 9, 36866, 8061, 5260,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>0</td>\n",
       "      <td>[10, 159, 5, 26, 5, 2930, 15, 1, 858, 875, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>0</td>\n",
       "      <td>[57, 28, 5596, 1, 328, 2005, 100, 5, 27, 310, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Label Encode                                          sequences\n",
       "0                 1  [28, 4, 1, 79, 1940, 45, 1025, 12, 99, 142, 40...\n",
       "1                 1  [3, 370, 118, 351, 7, 7, 1, 1321, 2928, 6, 52,...\n",
       "2                 1  [10, 187, 11, 13, 3, 370, 96, 5, 1072, 60, 21,...\n",
       "3                 0  [641, 38, 3, 223, 111, 3, 118, 405, 3192, 1166...\n",
       "4                 1  [60692, 10349, 112, 9, 1, 60, 4, 281, 6, 3, 20...\n",
       "...             ...                                                ...\n",
       "49995             1  [10, 187, 11, 17, 70, 3, 180, 195, 49, 282, 8,...\n",
       "49996             0  [84, 114, 84, 392, 84, 113, 2888, 926, 1, 605,...\n",
       "49997             0  [10, 222, 3, 3380, 4195, 9, 36866, 8061, 5260,...\n",
       "49998             0  [10, 159, 5, 26, 5, 2930, 15, 1, 858, 875, 2, ...\n",
       "49999             0  [57, 28, 5596, 1, 328, 2005, 100, 5, 27, 310, ...\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_df = df[['Label Encode', 'sequences']].copy()\n",
    "tokenized_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Split data in test and training\n",
    "\n",
    "Now I'll split the data in test and training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_y, test_y, train_x, test_x = train_test_split(tokenized_df['Label Encode'], np.array(input_ids), test_size=0.2, random_state=25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "Now I'm going to build the model. The labels are as follows:\n",
    "- 0. Negative\n",
    "- 1. Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  157  2874  4510 ...    50    11    19]\n",
      " [   10    76   798 ...   919    12    10]\n",
      " [   89     4    30 ...     4   521    47]\n",
      " ...\n",
      " [ 5364     1   415 ...   571   607     1]\n",
      " [   10   187    11 ... 10368    21 78608]\n",
      " [   10   196    11 ...   343     2     8]]\n"
     ]
    }
   ],
   "source": [
    "#Dataloaders aanmaken van train en test data.\n",
    "def create_set(x, y):\n",
    "    dataset = TensorDataset(\n",
    "    torch.tensor(x), torch.tensor(y.values, dtype=torch.long)\n",
    "    )\n",
    "    sampler = RandomSampler(dataset)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, sampler=sampler, batch_size=32\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "train_dataloader = create_set(train_x, train_y)\n",
    "test_dataloader = create_set(test_x, test_y)\n",
    "\n",
    "print(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Implement Class\n",
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    # def forward(self, text, offsets):\n",
    "    #     embedded = self.embedding(text, offsets)\n",
    "    #     return self.fc(embedded)\n",
    "\n",
    "    def __call__(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am going to count all the unique words. I will need this later for the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Counting all unique words.\n",
    "# all_words = sum(df['tokenized'], [])\n",
    "# unique_words = list(set(all_words))\n",
    "\n",
    "# #Tellen van de unieke woorden\n",
    "# num_unique_words = len(unique_words)\n",
    "# num_words = len(all_words)\n",
    "\n",
    "# print(\"The number of all words together:\", num_words)\n",
    "# print(\"The number of all unique words:\", num_unique_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Num_class = the length of all unique labels<br>\n",
    "vocab_size = the length of unique words in the dataframe (review)<br>\n",
    "emsize = the length of the word embeddings<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of unique labels in the label encode column\n",
    "num_class = len(tokenized_df['Label Encode'].unique())\n",
    "# The model will be trained on a corpus with ..  unique words.\n",
    "vocab_size = 124048\n",
    "# embedding size, each word in the input will be\n",
    "# represented by 64-dimenstional vector\n",
    "emsize = 64\n",
    "# initialize a new text classification model\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (text, label) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        # Call the model without the `offsets` argument   \n",
    "        predicted_label = model(text)\n",
    "\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (text, label) in enumerate(dataloader):\n",
    "            # Call the model without the `offsets` argument\n",
    "            predicted_label = model(text)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::_foreach_norm.Scalar' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_foreach_norm.Scalar' is only available for these backends: [CPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at aten\\src\\ATen\\RegisterCPU.cpp:31034 [kernel]\nBackendSelect: fallthrough registered at ..\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:144 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:491 [backend fallback]\nFunctionalize: registered at ..\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:280 [backend fallback]\nNamed: registered at ..\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ..\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ..\\aten\\src\\ATen\\native\\NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ..\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradOther: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradCPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradCUDA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradHIP: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradXLA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradMPS: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradIPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradXPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradHPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradVE: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradLazy: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradMeta: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradMTIA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradPrivateUse1: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradPrivateUse2: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradPrivateUse3: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradNestedTensor: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nTracer: registered at ..\\torch\\csrc\\autograd\\generated\\TraceType_2.cpp:16726 [kernel]\nAutocastCPU: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:487 [backend fallback]\nAutocastCUDA: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:354 [backend fallback]\nFuncTorchBatched: registered at ..\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:815 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ..\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1073 [backend fallback]\nVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ..\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:152 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:487 [backend fallback]\nPythonDispatcher: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:148 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, EPOCHS \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m     27\u001b[0m     \u001b[39m# Train the model and collect the training accuracy\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     epoch_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> 29\u001b[0m     train(train_dataloader)\n\u001b[0;32m     30\u001b[0m     train_acc \u001b[39m=\u001b[39m evaluate(train_dataloader)\n\u001b[0;32m     31\u001b[0m     train_accuracies\u001b[39m.\u001b[39mappend(train_acc)\n",
      "Cell \u001b[1;32mIn[78], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader)\u001b[0m\n\u001b[0;32m     12\u001b[0m loss \u001b[39m=\u001b[39m criterion(predicted_label, label)\n\u001b[0;32m     13\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> 14\u001b[0m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mclip_grad_norm_(model\u001b[39m.\u001b[39;49mparameters(), \u001b[39m0.1\u001b[39;49m)\n\u001b[0;32m     15\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     16\u001b[0m total_acc \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (predicted_label\u001b[39m.\u001b[39margmax(\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m label)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Warmtebron\\anaconda3\\envs\\chessEnv\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:55\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[1;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m ((device, _), [grads]) \u001b[39min\u001b[39;00m grouped_grads\u001b[39m.\u001b[39mitems():\n\u001b[0;32m     54\u001b[0m     \u001b[39mif\u001b[39;00m (foreach \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m foreach) \u001b[39mand\u001b[39;00m _has_foreach_support(grads, device\u001b[39m=\u001b[39mdevice):\n\u001b[1;32m---> 55\u001b[0m         norms\u001b[39m.\u001b[39mextend(torch\u001b[39m.\u001b[39;49m_foreach_norm(grads, norm_type))\n\u001b[0;32m     56\u001b[0m     \u001b[39melif\u001b[39;00m foreach:\n\u001b[0;32m     57\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mforeach=True was passed, but can\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mt use the foreach API on \u001b[39m\u001b[39m{\u001b[39;00mdevice\u001b[39m.\u001b[39mtype\u001b[39m}\u001b[39;00m\u001b[39m tensors\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Could not run 'aten::_foreach_norm.Scalar' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_foreach_norm.Scalar' is only available for these backends: [CPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at aten\\src\\ATen\\RegisterCPU.cpp:31034 [kernel]\nBackendSelect: fallthrough registered at ..\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:144 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:491 [backend fallback]\nFunctionalize: registered at ..\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:280 [backend fallback]\nNamed: registered at ..\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ..\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ..\\aten\\src\\ATen\\native\\NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ..\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradOther: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradCPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradCUDA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradHIP: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradXLA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradMPS: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradIPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradXPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradHPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradVE: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradLazy: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradMeta: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradMTIA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradPrivateUse1: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradPrivateUse2: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradPrivateUse3: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nAutogradNestedTensor: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:17472 [autograd kernel]\nTracer: registered at ..\\torch\\csrc\\autograd\\generated\\TraceType_2.cpp:16726 [kernel]\nAutocastCPU: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:487 [backend fallback]\nAutocastCUDA: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:354 [backend fallback]\nFuncTorchBatched: registered at ..\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:815 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ..\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1073 [backend fallback]\nVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ..\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:152 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:487 [backend fallback]\nPythonDispatcher: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:148 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10 # epoch\n",
    "LR = [0.01, 1, 2]  # learning rate\n",
    "BATCH_SIZE = 43 # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "accuracy_lr = []\n",
    "\n",
    "\n",
    "\n",
    "# Collect the accuracy values at each epoch\n",
    "\n",
    "for lr in LR:\n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "  total_accu = None\n",
    "\n",
    "  train_accuracies = []\n",
    "  val_accuracies = []\n",
    "\n",
    "  for epoch in range(1, EPOCHS + 1):\n",
    "      # Train the model and collect the training accuracy\n",
    "      epoch_start_time = time.time()\n",
    "      train(train_dataloader)\n",
    "      train_acc = evaluate(train_dataloader)\n",
    "      train_accuracies.append(train_acc)\n",
    "      \n",
    "      # Evaluate the model and collect the validation accuracy\n",
    "      val_acc = evaluate(test_dataloader)\n",
    "      val_accuracies.append(val_acc)\n",
    "      \n",
    "      # Adjust the learning rate if the validation accuracy did not improve\n",
    "      if total_accu is not None and total_accu > val_acc:\n",
    "        scheduler.step()\n",
    "      else:\n",
    "        total_accu = val_acc\n",
    "      \n",
    "      print('-' * 59)\n",
    "      print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "            'train accuracy {:8.3f} | val accuracy {:8.3f}'.format(epoch,\n",
    "                                                                  time.time() - epoch_start_time,\n",
    "                                                                  train_acc, val_acc))\n",
    "      print('-' * 59)\n",
    " \n",
    "  accuracy_lr.append(val_acc)\n",
    "\n",
    "print(accuracy_lr)\n",
    "\n",
    "# Plot the training and validation accuracies\n",
    "plt.plot(train_accuracies, label='Training accuracy')\n",
    "plt.plot(val_accuracies, label='Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
