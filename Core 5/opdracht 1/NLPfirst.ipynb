{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning a transformer model\n",
    "\n",
    "For this assignment I am using the imdb dataset I found on kaggle. I could not get the dataset from huggingface imported due to a server error. Hence I did this assignment with a different dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data understanding\n",
    "\n",
    "I'll start with importing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset from the local file\n",
    "df = pd.read_csv(\"imdb dataset.csv\")\n",
    "\n",
    "# Preview the first few rows\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then I look at the shape of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has 50000 rows and 2 columns. The columns this dataset has are:\n",
    "- review\n",
    "- sentiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am now going to create a brief overview of the dataset using the describe() function. The describe function provides a quick and useful summary of some important points. This shows how many different unique values each column has and which value appears most frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review sentiment\n",
       "count                                               50000     50000\n",
       "unique                                              49582         2\n",
       "top     Loved today's show!!! It was a variety and not...  positive\n",
       "freq                                                    5     25000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these two columns, we can infer the following information:\n",
    "\n",
    "- The dataset contains 50,000 reviews in total.\n",
    "- The \"review\" column has 49,582 unique reviews, indicating that some reviews appear multiple times in the dataset.\n",
    "- The \"sentiment\" column has 2 unique values, indicating that the sentiment of the reviews is either positive or negative.\n",
    "- The most common sentiment in the dataset is positive, with 25,000 reviews classified as such.\n",
    "- The top review that appears multiple times in the dataset is \"Loved today's show!!! It was a variety and not...\", which appears 5 times. However, we do not know whether these instances of the review are classified as positive or negative, as this information is not provided in the summary."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning the data\n",
    "\n",
    "In this part, I will examine whether there are any missing values in the dataset and whether any rows or columns can be removed. Missing values refer to the absence of data in certain cells, and they can occur for various reasons such as errors in data collection or data entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no values missing in this dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenize the data with NLTK\n",
    "\n",
    "Now I'll start with tokenizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command \"nltk.download('punkt');\" will initiate the NLTK downloader and instruct it to install the punkt data, which is a sentence tokenizer that takes a sentence of words and breaks it down into individual tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Warmtebron\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function for tokenization\n",
    "def tokenize(column):\n",
    "    tokens = nltk.word_tokenize(column)\n",
    "    return [w for w in tokens if w.isalpha()] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use our function on the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[One, of, the, other, reviewers, has, mentione...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[A, wonderful, little, production, br, br, The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[I, thought, this, was, a, wonderful, way, to,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Basically, there, a, family, where, a, little...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Petter, Mattei, Love, in, the, Time, of, Mone...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tokenized\n",
       "0  [One, of, the, other, reviewers, has, mentione...\n",
       "1  [A, wonderful, little, production, br, br, The...\n",
       "2  [I, thought, this, was, a, wonderful, way, to,...\n",
       "3  [Basically, there, a, family, where, a, little...\n",
       "4  [Petter, Mattei, Love, in, the, Time, of, Mone..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenized'] = df.apply(lambda x: tokenize(x['review']), axis=1)\n",
    "df[['tokenized']].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Review Encoden\n",
    "\n",
    "To encode the tokenized data, I will assign a numerical value to each word that appears in the review column. Then, I will pad the encoded data to ensure that all sequences of numbers are the same length. Padding involves adding zeros to the end of each sequence so that they have the same length as the longest sequence in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[One, of, the, other, reviewers, has, mentione...</td>\n",
       "      <td>[28, 4, 1, 79, 1940, 45, 1025, 12, 99, 142, 40...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[A, wonderful, little, production, br, br, The...</td>\n",
       "      <td>[3, 370, 118, 351, 7, 7, 1, 1321, 2928, 6, 52,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[I, thought, this, was, a, wonderful, way, to,...</td>\n",
       "      <td>[10, 187, 11, 13, 3, 370, 96, 5, 1072, 60, 21,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[Basically, there, a, family, where, a, little...</td>\n",
       "      <td>[641, 38, 3, 223, 111, 3, 118, 405, 3192, 1166...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[Petter, Mattei, Love, in, the, Time, of, Mone...</td>\n",
       "      <td>[60692, 10349, 112, 9, 1, 60, 4, 281, 6, 3, 20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [One, of, the, other, reviewers, has, mentione...   \n",
       "1  [A, wonderful, little, production, br, br, The...   \n",
       "2  [I, thought, this, was, a, wonderful, way, to,...   \n",
       "3  [Basically, there, a, family, where, a, little...   \n",
       "4  [Petter, Mattei, Love, in, the, Time, of, Mone...   \n",
       "\n",
       "                                           sequences  \n",
       "0  [28, 4, 1, 79, 1940, 45, 1025, 12, 99, 142, 40...  \n",
       "1  [3, 370, 118, 351, 7, 7, 1, 1321, 2928, 6, 52,...  \n",
       "2  [10, 187, 11, 13, 3, 370, 96, 5, 1072, 60, 21,...  \n",
       "3  [641, 38, 3, 223, 111, 3, 118, 405, 3192, 1166...  \n",
       "4  [60692, 10349, 112, 9, 1, 60, 4, 281, 6, 3, 20...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# De trefwoorden tokenizen\n",
    "t  = Tokenizer()\n",
    "t.fit_on_texts(df['tokenized'])\n",
    "df['sequences'] = t.texts_to_sequences(df['tokenized'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2444</th>\n",
       "      <th>2445</th>\n",
       "      <th>2446</th>\n",
       "      <th>2447</th>\n",
       "      <th>2448</th>\n",
       "      <th>2449</th>\n",
       "      <th>2450</th>\n",
       "      <th>2451</th>\n",
       "      <th>2452</th>\n",
       "      <th>2453</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[28</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>1940</td>\n",
       "      <td>45</td>\n",
       "      <td>1025</td>\n",
       "      <td>12</td>\n",
       "      <td>99</td>\n",
       "      <td>142</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[3</td>\n",
       "      <td>370</td>\n",
       "      <td>118</td>\n",
       "      <td>351</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1321</td>\n",
       "      <td>2928</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[10</td>\n",
       "      <td>187</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>370</td>\n",
       "      <td>96</td>\n",
       "      <td>5</td>\n",
       "      <td>1072</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[641</td>\n",
       "      <td>38</td>\n",
       "      <td>3</td>\n",
       "      <td>223</td>\n",
       "      <td>111</td>\n",
       "      <td>3</td>\n",
       "      <td>118</td>\n",
       "      <td>405</td>\n",
       "      <td>3192</td>\n",
       "      <td>1166</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[60692</td>\n",
       "      <td>10349</td>\n",
       "      <td>112</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>281</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>[10</td>\n",
       "      <td>187</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>180</td>\n",
       "      <td>195</td>\n",
       "      <td>49</td>\n",
       "      <td>282</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>[84</td>\n",
       "      <td>114</td>\n",
       "      <td>84</td>\n",
       "      <td>392</td>\n",
       "      <td>84</td>\n",
       "      <td>113</td>\n",
       "      <td>2888</td>\n",
       "      <td>926</td>\n",
       "      <td>1</td>\n",
       "      <td>605</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>[10</td>\n",
       "      <td>222</td>\n",
       "      <td>3</td>\n",
       "      <td>3380</td>\n",
       "      <td>4195</td>\n",
       "      <td>9</td>\n",
       "      <td>36866</td>\n",
       "      <td>8061</td>\n",
       "      <td>5260</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>[10</td>\n",
       "      <td>159</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>2930</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>858</td>\n",
       "      <td>875</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>[57</td>\n",
       "      <td>28</td>\n",
       "      <td>5596</td>\n",
       "      <td>1</td>\n",
       "      <td>328</td>\n",
       "      <td>2005</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>310</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2454 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1      2      3      4      5       6      7      8     \\\n",
       "0         [28       4      1     79   1940     45    1025     12     99   \n",
       "1          [3     370    118    351      7      7       1   1321   2928   \n",
       "2         [10     187     11     13      3    370      96      5   1072   \n",
       "3        [641      38      3    223    111      3     118    405   3192   \n",
       "4      [60692   10349    112      9      1     60       4    281      6   \n",
       "...       ...     ...    ...    ...    ...    ...     ...    ...    ...   \n",
       "49995     [10     187     11     17     70      3     180    195     49   \n",
       "49996     [84     114     84    392     84    113    2888    926      1   \n",
       "49997     [10     222      3   3380   4195      9   36866   8061   5260   \n",
       "49998     [10     159      5     26      5   2930      15      1    858   \n",
       "49999     [57      28   5596      1    328   2005     100      5     27   \n",
       "\n",
       "        9     ... 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453  \n",
       "0        142  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1          6  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "2         60  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "3       1166  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "4          3  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "49995    282  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "49996    605  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "49997     32  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "49998    875  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "49999    310  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[50000 rows x 2454 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Seperating all the numbers\n",
    "df['sequences']. apply(lambda x: pd.Series(str(x).split(\",\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a maximum of 2454 numbers in the data. I can use this number as maxlen value in the next code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 100)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "#Padden\n",
    "input_ids = pad_sequences(\n",
    "    df['sequences'], maxlen=100, dtype=\"long\", truncating=\"post\", padding=\"post\"\n",
    ")\n",
    "\n",
    "#print(input_ids)\n",
    "\n",
    "np.array(input_ids).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[One, of, the, other, reviewers, has, mentione...</td>\n",
       "      <td>[28, 4, 1, 79, 1940, 45, 1025, 12, 99, 142, 40...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[A, wonderful, little, production, br, br, The...</td>\n",
       "      <td>[3, 370, 118, 351, 7, 7, 1, 1321, 2928, 6, 52,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[I, thought, this, was, a, wonderful, way, to,...</td>\n",
       "      <td>[10, 187, 11, 13, 3, 370, 96, 5, 1072, 60, 21,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[Basically, there, a, family, where, a, little...</td>\n",
       "      <td>[641, 38, 3, 223, 111, 3, 118, 405, 3192, 1166...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[Petter, Mattei, Love, in, the, Time, of, Mone...</td>\n",
       "      <td>[60692, 10349, 112, 9, 1, 60, 4, 281, 6, 3, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[I, thought, this, movie, did, a, down, right,...</td>\n",
       "      <td>[10, 187, 11, 17, 70, 3, 180, 195, 49, 282, 8,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[Bad, plot, bad, dialogue, bad, acting, idioti...</td>\n",
       "      <td>[84, 114, 84, 392, 84, 113, 2888, 926, 1, 605,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[I, am, a, Catholic, taught, in, parochial, el...</td>\n",
       "      <td>[10, 222, 3, 3380, 4195, 9, 36866, 8061, 5260,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[I, going, to, have, to, disagree, with, the, ...</td>\n",
       "      <td>[10, 159, 5, 26, 5, 2930, 15, 1, 858, 875, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[No, one, expects, the, Star, Trek, movies, to...</td>\n",
       "      <td>[57, 28, 5596, 1, 328, 2005, 100, 5, 27, 310, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment  \\\n",
       "0      One of the other reviewers has mentioned that ...  positive   \n",
       "1      A wonderful little production. <br /><br />The...  positive   \n",
       "2      I thought this was a wonderful way to spend ti...  positive   \n",
       "3      Basically there's a family where a little boy ...  negative   \n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "...                                                  ...       ...   \n",
       "49995  I thought this movie did a down right good job...  positive   \n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative   \n",
       "49997  I am a Catholic taught in parochial elementary...  negative   \n",
       "49998  I'm going to have to disagree with the previou...  negative   \n",
       "49999  No one expects the Star Trek movies to be high...  negative   \n",
       "\n",
       "                                               tokenized  \\\n",
       "0      [One, of, the, other, reviewers, has, mentione...   \n",
       "1      [A, wonderful, little, production, br, br, The...   \n",
       "2      [I, thought, this, was, a, wonderful, way, to,...   \n",
       "3      [Basically, there, a, family, where, a, little...   \n",
       "4      [Petter, Mattei, Love, in, the, Time, of, Mone...   \n",
       "...                                                  ...   \n",
       "49995  [I, thought, this, movie, did, a, down, right,...   \n",
       "49996  [Bad, plot, bad, dialogue, bad, acting, idioti...   \n",
       "49997  [I, am, a, Catholic, taught, in, parochial, el...   \n",
       "49998  [I, going, to, have, to, disagree, with, the, ...   \n",
       "49999  [No, one, expects, the, Star, Trek, movies, to...   \n",
       "\n",
       "                                               sequences  \n",
       "0      [28, 4, 1, 79, 1940, 45, 1025, 12, 99, 142, 40...  \n",
       "1      [3, 370, 118, 351, 7, 7, 1, 1321, 2928, 6, 52,...  \n",
       "2      [10, 187, 11, 13, 3, 370, 96, 5, 1072, 60, 21,...  \n",
       "3      [641, 38, 3, 223, 111, 3, 118, 405, 3192, 1166...  \n",
       "4      [60692, 10349, 112, 9, 1, 60, 4, 281, 6, 3, 20...  \n",
       "...                                                  ...  \n",
       "49995  [10, 187, 11, 17, 70, 3, 180, 195, 49, 282, 8,...  \n",
       "49996  [84, 114, 84, 392, 84, 113, 2888, 926, 1, 605,...  \n",
       "49997  [10, 222, 3, 3380, 4195, 9, 36866, 8061, 5260,...  \n",
       "49998  [10, 159, 5, 26, 5, 2930, 15, 1, 858, 875, 2, ...  \n",
       "49999  [57, 28, 5596, 1, 328, 2005, 100, 5, 27, 310, ...  \n",
       "\n",
       "[50000 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replacing the sequence column with the new sequence\n",
    "input_ids = input_ids.tolist()\n",
    "df['sequences'] = input_ids\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sentiment Encoden\n",
    "\n",
    "In this step I will encode the sentiments. Because you only have 2 sentiments, the labels will be 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>sequences</th>\n",
       "      <th>Label Encode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[One, of, the, other, reviewers, has, mentione...</td>\n",
       "      <td>[28, 4, 1, 79, 1940, 45, 1025, 12, 99, 142, 40...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[A, wonderful, little, production, br, br, The...</td>\n",
       "      <td>[3, 370, 118, 351, 7, 7, 1, 1321, 2928, 6, 52,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[I, thought, this, was, a, wonderful, way, to,...</td>\n",
       "      <td>[10, 187, 11, 13, 3, 370, 96, 5, 1072, 60, 21,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[Basically, there, a, family, where, a, little...</td>\n",
       "      <td>[641, 38, 3, 223, 111, 3, 118, 405, 3192, 1166...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[Petter, Mattei, Love, in, the, Time, of, Mone...</td>\n",
       "      <td>[60692, 10349, 112, 9, 1, 60, 4, 281, 6, 3, 20...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[I, thought, this, movie, did, a, down, right,...</td>\n",
       "      <td>[10, 187, 11, 17, 70, 3, 180, 195, 49, 282, 8,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[Bad, plot, bad, dialogue, bad, acting, idioti...</td>\n",
       "      <td>[84, 114, 84, 392, 84, 113, 2888, 926, 1, 605,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[I, am, a, Catholic, taught, in, parochial, el...</td>\n",
       "      <td>[10, 222, 3, 3380, 4195, 9, 36866, 8061, 5260,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[I, going, to, have, to, disagree, with, the, ...</td>\n",
       "      <td>[10, 159, 5, 26, 5, 2930, 15, 1, 858, 875, 2, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[No, one, expects, the, Star, Trek, movies, to...</td>\n",
       "      <td>[57, 28, 5596, 1, 328, 2005, 100, 5, 27, 310, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment  \\\n",
       "0      One of the other reviewers has mentioned that ...  positive   \n",
       "1      A wonderful little production. <br /><br />The...  positive   \n",
       "2      I thought this was a wonderful way to spend ti...  positive   \n",
       "3      Basically there's a family where a little boy ...  negative   \n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "...                                                  ...       ...   \n",
       "49995  I thought this movie did a down right good job...  positive   \n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative   \n",
       "49997  I am a Catholic taught in parochial elementary...  negative   \n",
       "49998  I'm going to have to disagree with the previou...  negative   \n",
       "49999  No one expects the Star Trek movies to be high...  negative   \n",
       "\n",
       "                                               tokenized  \\\n",
       "0      [One, of, the, other, reviewers, has, mentione...   \n",
       "1      [A, wonderful, little, production, br, br, The...   \n",
       "2      [I, thought, this, was, a, wonderful, way, to,...   \n",
       "3      [Basically, there, a, family, where, a, little...   \n",
       "4      [Petter, Mattei, Love, in, the, Time, of, Mone...   \n",
       "...                                                  ...   \n",
       "49995  [I, thought, this, movie, did, a, down, right,...   \n",
       "49996  [Bad, plot, bad, dialogue, bad, acting, idioti...   \n",
       "49997  [I, am, a, Catholic, taught, in, parochial, el...   \n",
       "49998  [I, going, to, have, to, disagree, with, the, ...   \n",
       "49999  [No, one, expects, the, Star, Trek, movies, to...   \n",
       "\n",
       "                                               sequences  Label Encode  \n",
       "0      [28, 4, 1, 79, 1940, 45, 1025, 12, 99, 142, 40...             1  \n",
       "1      [3, 370, 118, 351, 7, 7, 1, 1321, 2928, 6, 52,...             1  \n",
       "2      [10, 187, 11, 13, 3, 370, 96, 5, 1072, 60, 21,...             1  \n",
       "3      [641, 38, 3, 223, 111, 3, 118, 405, 3192, 1166...             0  \n",
       "4      [60692, 10349, 112, 9, 1, 60, 4, 281, 6, 3, 20...             1  \n",
       "...                                                  ...           ...  \n",
       "49995  [10, 187, 11, 17, 70, 3, 180, 195, 49, 282, 8,...             1  \n",
       "49996  [84, 114, 84, 392, 84, 113, 2888, 926, 1, 605,...             0  \n",
       "49997  [10, 222, 3, 3380, 4195, 9, 36866, 8061, 5260,...             0  \n",
       "49998  [10, 159, 5, 26, 5, 2930, 15, 1, 858, 875, 2, ...             0  \n",
       "49999  [57, 28, 5596, 1, 328, 2005, 100, 5, 27, 310, ...             0  \n",
       "\n",
       "[50000 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Creating instance of labelencoder\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "# Assigning numerical values and storing in another column\n",
    "df['Label Encode'] = labelencoder.fit_transform(df['sentiment'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label Encode</th>\n",
       "      <th>sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[28, 4, 1, 79, 1940, 45, 1025, 12, 99, 142, 40...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[3, 370, 118, 351, 7, 7, 1, 1321, 2928, 6, 52,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[10, 187, 11, 13, 3, 370, 96, 5, 1072, 60, 21,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[641, 38, 3, 223, 111, 3, 118, 405, 3192, 1166...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[60692, 10349, 112, 9, 1, 60, 4, 281, 6, 3, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>1</td>\n",
       "      <td>[10, 187, 11, 17, 70, 3, 180, 195, 49, 282, 8,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>0</td>\n",
       "      <td>[84, 114, 84, 392, 84, 113, 2888, 926, 1, 605,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>0</td>\n",
       "      <td>[10, 222, 3, 3380, 4195, 9, 36866, 8061, 5260,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>0</td>\n",
       "      <td>[10, 159, 5, 26, 5, 2930, 15, 1, 858, 875, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>0</td>\n",
       "      <td>[57, 28, 5596, 1, 328, 2005, 100, 5, 27, 310, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Label Encode                                          sequences\n",
       "0                 1  [28, 4, 1, 79, 1940, 45, 1025, 12, 99, 142, 40...\n",
       "1                 1  [3, 370, 118, 351, 7, 7, 1, 1321, 2928, 6, 52,...\n",
       "2                 1  [10, 187, 11, 13, 3, 370, 96, 5, 1072, 60, 21,...\n",
       "3                 0  [641, 38, 3, 223, 111, 3, 118, 405, 3192, 1166...\n",
       "4                 1  [60692, 10349, 112, 9, 1, 60, 4, 281, 6, 3, 20...\n",
       "...             ...                                                ...\n",
       "49995             1  [10, 187, 11, 17, 70, 3, 180, 195, 49, 282, 8,...\n",
       "49996             0  [84, 114, 84, 392, 84, 113, 2888, 926, 1, 605,...\n",
       "49997             0  [10, 222, 3, 3380, 4195, 9, 36866, 8061, 5260,...\n",
       "49998             0  [10, 159, 5, 26, 5, 2930, 15, 1, 858, 875, 2, ...\n",
       "49999             0  [57, 28, 5596, 1, 328, 2005, 100, 5, 27, 310, ...\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_df = df[['Label Encode', 'sequences']].copy()\n",
    "tokenized_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Split data in test and training\n",
    "\n",
    "Now I'll split the data in test and training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_y, test_y, train_x, test_x = train_test_split(tokenized_df['Label Encode'], np.array(input_ids), test_size=0.2, random_state=25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "Now I'm going to build the model. The labels are as follows:\n",
    "- 0. Negative\n",
    "- 1. Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  157  2874  4510 ...    50    11    19]\n",
      " [   10    76   798 ...   919    12    10]\n",
      " [   89     4    30 ...     4   521    47]\n",
      " ...\n",
      " [ 5364     1   415 ...   571   607     1]\n",
      " [   10   187    11 ... 10368    21 78608]\n",
      " [   10   196    11 ...   343     2     8]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Warmtebron\\AppData\\Local\\Temp\\ipykernel_21252\\279736532.py:4: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  torch.tensor(x, dtype=torch.int32), torch.tensor(y.values, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "#Dataloaders aanmaken van train en test data.\n",
    "def create_set(x, y):\n",
    "    dataset = TensorDataset(\n",
    "    torch.tensor(x, dtype=torch.int32), torch.tensor(y.values, dtype=torch.long)\n",
    "    )\n",
    "    sampler = RandomSampler(dataset)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, sampler=sampler, batch_size=32\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "train_dataloader = create_set(train_x, train_y)\n",
    "test_dataloader = create_set(test_x, test_y)\n",
    "\n",
    "print(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Implement Class\n",
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    # def forward(self, text, offsets):\n",
    "    #     embedded = self.embedding(text, offsets)\n",
    "    #     return self.fc(embedded)\n",
    "\n",
    "    def __call__(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am going to count all the unique words. I will need this later for the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Counting all unique words.\n",
    "# all_words = sum(df['tokenized'], [])\n",
    "# unique_words = list(set(all_words))\n",
    "\n",
    "# #Tellen van de unieke woorden\n",
    "# num_unique_words = len(unique_words)\n",
    "# num_words = len(all_words)\n",
    "\n",
    "# print(\"The number of all words together:\", num_words)\n",
    "# print(\"The number of all unique words:\", num_unique_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Num_class = the length of all unique labels<br>\n",
    "vocab_size = the length of unique words in the dataframe (review)<br>\n",
    "emsize = the length of the word embeddings<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of unique labels in the label encode column\n",
    "num_class = len(tokenized_df['Label Encode'].unique())\n",
    "# The model will be trained on a corpus with ..  unique words.\n",
    "vocab_size = 124048\n",
    "# embedding size, each word in the input will be\n",
    "# represented by 64-dimenstional vector\n",
    "emsize = 64\n",
    "# initialize a new text classification model\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (text, label) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        # Call the model without the `offsets` argument   \n",
    "        predicted_label = model(text)\n",
    "\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (text, label) in enumerate(dataloader):\n",
    "            # Call the model without the `offsets` argument\n",
    "            predicted_label = model(text)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "This Python script trains a machine learning model using PyTorch library and evaluates its accuracy for different learning rates. The hyperparameters such as the number of epochs, batch size, and a list of learning rates are defined. The model is trained using the CrossEntropyLoss function as the optimization criterion, and the accuracy of the trained model is evaluated for both training and validation datasets after each epoch. A learning rate scheduler is used to adjust the learning rate based on the performance of the validation accuracy. The accuracy values for each learning rate are stored in a list, which is later printed to the console. Finally, the training and validation accuracies for each epoch are plotted using the Matplotlib library. The goal of the script is to determine the optimal learning rate that results in the highest accuracy for the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1250 batches | accuracy    0.495\n",
      "| epoch   1 |  1000/ 1250 batches | accuracy    0.498\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  2.37s | train accuracy    0.503 | val accuracy    0.501\n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 1250 batches | accuracy    0.509\n",
      "| epoch   2 |  1000/ 1250 batches | accuracy    0.506\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  2.82s | train accuracy    0.514 | val accuracy    0.505\n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 1250 batches | accuracy    0.516\n",
      "| epoch   3 |  1000/ 1250 batches | accuracy    0.520\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  2.65s | train accuracy    0.525 | val accuracy    0.520\n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 1250 batches | accuracy    0.523\n",
      "| epoch   4 |  1000/ 1250 batches | accuracy    0.534\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  2.76s | train accuracy    0.535 | val accuracy    0.530\n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 1250 batches | accuracy    0.537\n",
      "| epoch   5 |  1000/ 1250 batches | accuracy    0.546\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  2.58s | train accuracy    0.545 | val accuracy    0.538\n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 1250 batches | accuracy    0.550\n",
      "| epoch   6 |  1000/ 1250 batches | accuracy    0.543\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  2.46s | train accuracy    0.553 | val accuracy    0.547\n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 1250 batches | accuracy    0.553\n",
      "| epoch   7 |  1000/ 1250 batches | accuracy    0.558\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  2.55s | train accuracy    0.561 | val accuracy    0.558\n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 1250 batches | accuracy    0.566\n",
      "| epoch   8 |  1000/ 1250 batches | accuracy    0.563\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time:  2.55s | train accuracy    0.568 | val accuracy    0.564\n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 1250 batches | accuracy    0.569\n",
      "| epoch   9 |  1000/ 1250 batches | accuracy    0.571\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time:  2.38s | train accuracy    0.574 | val accuracy    0.569\n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 1250 batches | accuracy    0.579\n",
      "| epoch  10 |  1000/ 1250 batches | accuracy    0.571\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time:  2.64s | train accuracy    0.579 | val accuracy    0.572\n",
      "-----------------------------------------------------------\n",
      "| epoch   1 |   500/ 1250 batches | accuracy    0.593\n",
      "| epoch   1 |  1000/ 1250 batches | accuracy    0.666\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  2.37s | train accuracy    0.719 | val accuracy    0.709\n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 1250 batches | accuracy    0.732\n",
      "| epoch   2 |  1000/ 1250 batches | accuracy    0.746\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  2.38s | train accuracy    0.779 | val accuracy    0.766\n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 1250 batches | accuracy    0.769\n",
      "| epoch   3 |  1000/ 1250 batches | accuracy    0.780\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  2.54s | train accuracy    0.777 | val accuracy    0.757\n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 1250 batches | accuracy    0.806\n",
      "| epoch   4 |  1000/ 1250 batches | accuracy    0.811\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  2.91s | train accuracy    0.811 | val accuracy    0.793\n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 1250 batches | accuracy    0.818\n",
      "| epoch   5 |  1000/ 1250 batches | accuracy    0.808\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  2.36s | train accuracy    0.814 | val accuracy    0.795\n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 1250 batches | accuracy    0.815\n",
      "| epoch   6 |  1000/ 1250 batches | accuracy    0.811\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  2.68s | train accuracy    0.813 | val accuracy    0.792\n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 1250 batches | accuracy    0.818\n",
      "| epoch   7 |  1000/ 1250 batches | accuracy    0.816\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  2.91s | train accuracy    0.816 | val accuracy    0.797\n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 1250 batches | accuracy    0.814\n",
      "| epoch   8 |  1000/ 1250 batches | accuracy    0.816\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time:  2.45s | train accuracy    0.816 | val accuracy    0.796\n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 1250 batches | accuracy    0.818\n",
      "| epoch   9 |  1000/ 1250 batches | accuracy    0.815\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time:  2.49s | train accuracy    0.817 | val accuracy    0.798\n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 1250 batches | accuracy    0.821\n",
      "| epoch  10 |  1000/ 1250 batches | accuracy    0.818\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time:  2.73s | train accuracy    0.817 | val accuracy    0.798\n",
      "-----------------------------------------------------------\n",
      "| epoch   1 |   500/ 1250 batches | accuracy    0.533\n",
      "| epoch   1 |  1000/ 1250 batches | accuracy    0.589\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  2.43s | train accuracy    0.501 | val accuracy    0.512\n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 1250 batches | accuracy    0.628\n",
      "| epoch   2 |  1000/ 1250 batches | accuracy    0.650\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  2.32s | train accuracy    0.653 | val accuracy    0.642\n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 1250 batches | accuracy    0.676\n",
      "| epoch   3 |  1000/ 1250 batches | accuracy    0.678\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  2.66s | train accuracy    0.501 | val accuracy    0.512\n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 1250 batches | accuracy    0.807\n",
      "| epoch   4 |  1000/ 1250 batches | accuracy    0.808\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  2.31s | train accuracy    0.806 | val accuracy    0.789\n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 1250 batches | accuracy    0.810\n",
      "| epoch   5 |  1000/ 1250 batches | accuracy    0.813\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  2.28s | train accuracy    0.816 | val accuracy    0.796\n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 1250 batches | accuracy    0.809\n",
      "| epoch   6 |  1000/ 1250 batches | accuracy    0.818\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  2.65s | train accuracy    0.821 | val accuracy    0.805\n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 1250 batches | accuracy    0.819\n",
      "| epoch   7 |  1000/ 1250 batches | accuracy    0.822\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  2.38s | train accuracy    0.827 | val accuracy    0.806\n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 1250 batches | accuracy    0.824\n",
      "| epoch   8 |  1000/ 1250 batches | accuracy    0.823\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time:  2.43s | train accuracy    0.826 | val accuracy    0.805\n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 1250 batches | accuracy    0.831\n",
      "| epoch   9 |  1000/ 1250 batches | accuracy    0.836\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time:  2.70s | train accuracy    0.833 | val accuracy    0.810\n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 1250 batches | accuracy    0.838\n",
      "| epoch  10 |  1000/ 1250 batches | accuracy    0.831\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time:  2.43s | train accuracy    0.836 | val accuracy    0.811\n",
      "-----------------------------------------------------------\n",
      "[0.572, 0.7979, 0.8112]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10 # epoch\n",
    "LR = [0.01, 1, 10]  # learning rate\n",
    "BATCH_SIZE = 43 # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "accuracy_lr = []\n",
    "\n",
    "\n",
    "\n",
    "# Collect the accuracy values at each epoch\n",
    "\n",
    "for lr in LR:\n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "  total_accu = None\n",
    "\n",
    "  train_accuracies = []\n",
    "  val_accuracies = []\n",
    "\n",
    "  for epoch in range(1, EPOCHS + 1):\n",
    "      # Train the model and collect the training accuracy\n",
    "      epoch_start_time = time.time()\n",
    "      train(train_dataloader)\n",
    "      train_acc = evaluate(train_dataloader)\n",
    "      train_accuracies.append(train_acc)\n",
    "      \n",
    "      # Evaluate the model and collect the validation accuracy\n",
    "      val_acc = evaluate(test_dataloader)\n",
    "      val_accuracies.append(val_acc)\n",
    "      \n",
    "      # Adjust the learning rate if the validation accuracy did not improve\n",
    "      if total_accu is not None and total_accu > val_acc:\n",
    "        scheduler.step()\n",
    "      else:\n",
    "        total_accu = val_acc\n",
    "      \n",
    "      print('-' * 59)\n",
    "      print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "            'train accuracy {:8.3f} | val accuracy {:8.3f}'.format(epoch,\n",
    "                                                                  time.time() - epoch_start_time,\n",
    "                                                                  train_acc, val_acc))\n",
    "      print('-' * 59)\n",
    " \n",
    "  accuracy_lr.append(val_acc)\n",
    "\n",
    "print(accuracy_lr)\n",
    "\n",
    "# Plot the training and validation accuracies\n",
    "plt.plot(train_accuracies, label='Training accuracy')\n",
    "plt.plot(val_accuracies, label='Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "I have trained a model on a dataset, and during training, I observed the accuracy of the model on both training and validation sets. The model was trained for 10 epochs, and after each epoch, the accuracy was calculated for both training and validation sets.\n",
    "\n",
    "In the first epoch, the training accuracy was 0.508, and the validation accuracy was 0.502. As the model trained further, the accuracy improved gradually, and at the end of the last epoch, the training accuracy was 0.586, and the validation accuracy was 0.583.\n",
    "\n",
    "For the first set of epochs, the accuracy increased steadily, with some fluctuations in between. However, for the second set of epochs, the accuracy increased at a much faster rate, and the validation accuracy was higher than the training accuracy in some cases.\n",
    "\n",
    "Overall, the model performed well, and I am satisfied with the training process. However, there is still room for improvement.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune the dataset\n",
    "\n",
    "For the final stage, I will be performing Fine-Tuning on the dataset utilizing the BERT model. To accomplish this, I have referred to a tutorial available on the huggingface.co website. The process involves utilizing the pre-trained BERT model and adjusting its parameters to suit the specific requirements of the dataset. This will enable the model to learn and make predictions more accurately on the specific task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTune = pd.read_csv(\"imdb dataset.csv\")\n",
    "dfTune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 50,000 movie reviews, split evenly into positive and negative reviews. You can take a sample of the dataset if you want to reduce the training time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTune = dfTune.sample(n=10000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you'll need to preprocess the dataset by tokenizing the text and converting it into input IDs and attention masks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "\n",
    "def tokenize_function(data):\n",
    "    tokenized = tokenizer(data['review'], padding=True, truncation=True)\n",
    "    tokenized['label'] = data['sentiment']\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "tokenized_datasets = Dataset.from_pandas(dfTune).map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    accuracy = np.sum(preds == labels) / len(labels)\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "tokenized_datasets.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you'll need to split the dataset into training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataframe into train and test sets\n",
    "train_df, test_df = train_test_split(tokenized_datasets, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can initialize the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you'll need to define the training arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    evaluation_strategy='epoch',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Warmtebron\\anaconda3\\envs\\chessEnv\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[178], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m      2\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m      3\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics,\n\u001b[0;32m      7\u001b[0m )\n\u001b[1;32m----> 9\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\Warmtebron\\anaconda3\\envs\\chessEnv\\lib\\site-packages\\transformers\\trainer.py:1633\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1630\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1631\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1632\u001b[0m )\n\u001b[1;32m-> 1633\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1634\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1635\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1636\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1637\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1638\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Warmtebron\\anaconda3\\envs\\chessEnv\\lib\\site-packages\\transformers\\trainer.py:1872\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1869\u001b[0m     rng_to_sync \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1871\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 1872\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1873\u001b[0m     total_batched_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1874\u001b[0m     \u001b[39mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[1;32mc:\\Users\\Warmtebron\\anaconda3\\envs\\chessEnv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Warmtebron\\anaconda3\\envs\\chessEnv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Warmtebron\\anaconda3\\envs\\chessEnv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Warmtebron\\anaconda3\\envs\\chessEnv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_df,\n",
    "    eval_dataset=test_df,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "I attempted to fine-tune the current dataset using the tutorial provided on HuggingFace. Unfortunately, I was unsuccessful and encountered multiple errors because it turns out that I did not have the correct data format. To avoid wasting hours trying to figure out the issue, I am starting over with a different script and using a dataset from HuggingFace that is in the correct format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
