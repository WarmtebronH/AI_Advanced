{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Tabular Q-learning with Gym\n",
    "\n",
    "Reinforcement learning is an exciting field that has seen tremendous progress in recent years. With the help of powerful tools like Gym, it has become easier to experiment with different reinforcement learning algorithms and environments. In this tutorial, we will be exploring one such algorithm, Tabular Q-learning, and training an agent on the FrozenLake environment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Understanding the FrozenLake enviroment \n",
    "\n",
    "The FrozenLake environment is a simple grid-world game where the goal is to navigate an agent from the starting point to the goal while avoiding holes in the ground. The agent can move up, down, left, or right, and receives a reward of 1 for reaching the goal and a reward of 0 for falling into a hole or moving into a wall. The environment is implemented in Gym and can be imported using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"FrozenLake-v1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Implementing the Tabular Q-learning algorithm\n",
    "\n",
    "Next, we need to implement the Tabular Q-learning algorithm. This algorithm works by estimating the optimal action-value function, Q(s,a), which is the expected cumulative reward for taking action a in state s and then following the optimal policy thereafter. The algorithm updates the Q-values using the following formula:\n",
    "\n",
    "Q(s,a) = Q(s,a) + alpha * (reward + gamma * max(Q(s',a')) - Q(s,a))\n",
    "\n",
    "Where alpha is the learning rate, gamma is the discount factor, and s' is the next state after taking action a in state s.\n",
    "\n",
    "Here's an implementation of the Tabular Q-learning algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_episodes):\n\u001b[0;32m     41\u001b[0m     \u001b[39m# Reset environment\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     obs \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m---> 43\u001b[0m     state \u001b[39m=\u001b[39m discretize(obs, n_states)\n\u001b[0;32m     44\u001b[0m     done \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     \u001b[39m# While episode is not finished\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 26\u001b[0m, in \u001b[0;36mdiscretize\u001b[1;34m(obs, n_states)\u001b[0m\n\u001b[0;32m     23\u001b[0m bin_width \u001b[39m=\u001b[39m obs_range \u001b[39m/\u001b[39m n_states  \u001b[39m# Width of each bin\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39m# Select relevant features of the observation\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m cart_pos \u001b[39m=\u001b[39m obs[\u001b[39m'\u001b[39;49m\u001b[39mcart_position\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m     27\u001b[0m cart_vel \u001b[39m=\u001b[39m obs[\u001b[39m'\u001b[39m\u001b[39mcart_velocity\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     28\u001b[0m pole_ang \u001b[39m=\u001b[39m obs[\u001b[39m'\u001b[39m\u001b[39mpole_angle\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Initialize environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Initialize Q-table with zeros\n",
    "n_states = 10  # Number of discrete states\n",
    "n_actions = env.action_space.n  # Number of actions\n",
    "Q = np.zeros([n_states, n_states, n_states, n_states, n_actions])\n",
    "\n",
    "# Set hyperparameters\n",
    "alpha = 0.8\n",
    "gamma = 0.95\n",
    "epsilon = 0.1\n",
    "num_episodes = 5000\n",
    "\n",
    "# Discretize observation space\n",
    "def discretize(obs, n_states):\n",
    "    obs_min = -1.0  # Minimum observation value\n",
    "    obs_max = 1.0   # Maximum observation value\n",
    "    obs_range = obs_max - obs_min  # Range of observation values\n",
    "    bin_width = obs_range / n_states  # Width of each bin\n",
    "    \n",
    "    # Select relevant features of the observation\n",
    "    cart_pos = obs['cart_position']\n",
    "    cart_vel = obs['cart_velocity']\n",
    "    pole_ang = obs['pole_angle']\n",
    "    pole_vel = obs['pole_velocity']\n",
    "    \n",
    "    # Map features to nearest state indices\n",
    "    state = (np.digitize(cart_pos, np.linspace(obs_min, obs_max, n_states)) - 1,\n",
    "             np.digitize(cart_vel, np.linspace(obs_min, obs_max, n_states)) - 1,\n",
    "             np.digitize(pole_ang, np.linspace(obs_min, obs_max, n_states)) - 1,\n",
    "             np.digitize(pole_vel, np.linspace(obs_min, obs_max, n_states)) - 1)\n",
    "    \n",
    "    return state\n",
    "\n",
    "# For each episode\n",
    "for i in range(num_episodes):\n",
    "    # Reset environment\n",
    "    obs = env.reset()\n",
    "    state = discretize(obs, n_states)\n",
    "    done = False\n",
    "    \n",
    "    # While episode is not finished\n",
    "    while not done:\n",
    "        # Choose action using epsilon-greedy policy\n",
    "        if np.random.uniform() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q[state])\n",
    "            \n",
    "        # Take action and observe next state and reward\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        next_state = discretize(obs, n_states)\n",
    "        \n",
    "        # Update Q-table\n",
    "        Q[state][action] = Q[state][action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state][action])\n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "# Print learned Q-table\n",
    "print(Q)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training the agent on FrozenLake\n",
    "\n",
    "Now that we have implemented the Tabular Q-learning algorithm, we can train an agent on the FrozenLake environment. Here's an implementation of the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-table with zeros\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Set hyperparameters\n",
    "alpha = 0.8\n",
    "gamma = 0.95\n",
    "epsilon = 0.1\n",
    "num_episodes = 5000\n",
    "\n",
    "# For each episode\n",
    "for i in range(num_episodes):\n",
    "    # Reset environment\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    # While episode is not finished\n",
    "    while not done:\n",
    "        # Choose action using epsilon-greedy policy\n",
    "        if np.random.uniform() < epsilon:\n",
    "            action = env.action_space\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q[state,:])\n",
    "            \n",
    "        # Take action and observe next state and reward\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Update Q-table\n",
    "        Q[state,action] = Q[state,action] + alpha * (reward + gamma * np.max(Q[next_state,:]) - Q[state,action])\n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "    # Print episode number every 1000 episodes\n",
    "    if i % 1000 == 0:\n",
    "        print(\"Episode \", i)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluating the agent\n",
    "\n",
    "After training the agent, we can evaluate its performance by running several episodes and computing the average reward. Here's an implementation of the evaluation loop:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 100 episodes\n",
    "num_episodes = 100\n",
    "total_reward = 0\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    # Reset environment\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    # While episode is not finished\n",
    "    while not done:\n",
    "        # Choose action using greedy policy\n",
    "        action = np.argmax(Q[state,:])\n",
    "            \n",
    "        # Take action and observe next state and reward\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Update total reward\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "    # Print episode number every 10 episodes\n",
    "    if i % 10 == 0:\n",
    "        print(\"Episode \", i)\n",
    "        \n",
    "# Print average reward\n",
    "print(\"Average reward: \", total_reward/num_episodes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chessEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
