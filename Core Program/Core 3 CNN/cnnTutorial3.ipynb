{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning is a powerful technique that involves using pre-trained models as a starting point for developing new models. The idea is to leverage the knowledge and expertise learned by a pre-trained model on a large dataset to improve the accuracy and efficiency of a new model on a smaller dataset.\n",
    "\n",
    "Convolutional neural networks (CNNs) are a type of deep learning model that are commonly used in computer vision tasks, such as image classification, object detection, and segmentation. CNNs consist of multiple layers, including convolutional layers, pooling layers, and fully connected layers.\n",
    "\n",
    "In this tutorial, we'll use transfer learning to develop a CNN model for image classification. We'll use the VGG16 pre-trained model as our starting point, which was trained on the ImageNet dataset containing millions of images across thousands of classes.\n",
    "\n",
    "Here are the steps we'll follow:\n",
    "\n",
    "1. Load the VGG16 pre-trained model\n",
    "2. Freeze the pre-trained layers\n",
    "3. Add new layers for our specific task\n",
    "4. Compile and train the model\n",
    "5. Evaluate the model on a test set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the VGG16 pre-trained model\n",
    "\n",
    "First, we'll load the pre-trained VGG16 model from the Keras library. Keras is a popular deep learning library that provides a simple and intuitive interface for building and training deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "# Load the VGG16 model with pre-trained weights\n",
    "model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VGG16 function takes several arguments:\n",
    "\n",
    "- weights: specifies the pre-trained weights to use. We'll use the 'imagenet' weights, which were trained on the ImageNet dataset.\n",
    "- include_top: specifies whether to include the fully connected layers at the top of the model. We'll set this to False, since we'll be adding our own fully connected layers later.\n",
    "- input_shape: specifies the shape of the input images. VGG16 expects images with dimensions of 224x224 pixels and 3 channels (RGB)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Freeze the pre-trained layers\n",
    "Next, we'll freeze the pre-trained layers so that they're not updated during training. This is important because we want to leverage the knowledge learned by the pre-trained model, and only update the new layers that we add for our specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the pre-trained layers\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loop through each layer in the model and set its trainable attribute to False. This prevents the weights in those layers from being updated during training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Add new layers for our specific task\n",
    "Now that we have the pre-trained model loaded and its layers frozen, we can add our own fully connected layers on top of it for our specific task. In this example, we'll add a global average pooling layer followed by a fully connected layer with a softmax activation function for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "# Add new layers on top of the pre-trained model\n",
    "model_top = Sequential()\n",
    "model_top.add(GlobalAveragePooling2D(input_shape=model.output_shape[1:]))\n",
    "model_top.add(Dense(256, activation='relu'))\n",
    "model_top.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Combine the pre-trained model and the new layers\n",
    "model_combined = Sequential()\n",
    "model_combined.add(model)\n",
    "model_combined.add(model_top)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added a GlobalAveragePooling2D layer to the model, which reduces the spatial dimensions of the output from the pre-trained model to a 1D feature vector. This is followed by a Dense layer with 256 neurons and a ReLU activation function. The final Dense layer has 10 neurons (assuming we're building a model for classifying 10 different classes), and a softmax activation function for classification.\n",
    "\n",
    "We then combine the pre-trained model and the new layers using the Sequential API from Keras. This creates a new model with the pre-trained layers frozen and the new layers added on top."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Compile and train the model\n",
    "Now that we have our model defined, we can compile it and train it on our dataset. We'll use the compile method to specify the loss function, optimizer, and evaluation metric for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2168\\997635585.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel_combined\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_images' is not defined"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model_combined.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_combined.fit(train_images, train_labels, batch_size=32, epochs=10, validation_data=(test_images, test_labels))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model on a test set\n",
    "After training the model, we can evaluate its performance on a test set to see how well it generalizes to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12376\\1149977847.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Evaluate the model on the test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_combined\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test accuracy:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Warmtebron\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Warmtebron\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_assert_compile_was_called\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3688\u001b[0m         \u001b[1;31m# (i.e. whether the model is built and its inputs/outputs are set).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3689\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3690\u001b[1;33m             raise RuntimeError(\n\u001b[0m\u001b[0;32m   3691\u001b[0m                 \u001b[1;34m\"You must compile your model before \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3692\u001b[0m                 \u001b[1;34m\"training/testing. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model_combined.evaluate(test_images, test_labels)\n",
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the evaluate method to compute the loss and accuracy of the model on the test set. The results are printed to the console."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "That's it! In this tutorial, we learned how to use transfer learning to develop a CNN model for image classification. We loaded the pre-trained VGG16 model, froze its layers, added our own fully connected layers on top, compiled and trained the model, and evaluated its performance on a test set. Transfer learning is a powerful technique that can save time and improve the accuracy of deep learning models, especially when working with limited amounts of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "135dd61f25e7ad99382e31a9b18da1fa31b0780841b4ca2974d870836fb3c00a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
